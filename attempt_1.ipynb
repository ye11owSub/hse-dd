{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83487a33-a389-4871-8441-63cde1581073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms as t\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from hse_dd.src.utils import Dataset, download_dataset, process_dataset\n",
    "from hse_dd.src.nn import YoloDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0ee999-28aa-46be-951a-d1166fda93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = Path(\"./share/raw_data\")\n",
    "OUTPUT_DIR = Path(\"./share/combined_dataset\")\n",
    "ANNOTATIONS_DIR = OUTPUT_DIR / \"annotations\"\n",
    "IMAGES_DIR = OUTPUT_DIR / \"images\"\n",
    "ANNOTATIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datasets = [\n",
    "    Dataset(\n",
    "        name=\"dasmehdixtr\",\n",
    "        base_path=datasets_dir / \"dasmehdixtr\" / \"drone_dataset_yolo\" / \"dataset_txt\"\n",
    "    ),\n",
    "    Dataset(\n",
    "        name=\"dasmehdixtr\",\n",
    "        meta_type=\"xml\",\n",
    "        base_path=datasets_dir / \"dasmehdixtr\" / \"dataset_xml_format\" / \"dataset_xml_format\"\n",
    "    ),\n",
    "    Dataset(\n",
    "        name=\"mcagriaksoy\",\n",
    "        base_path=datasets_dir / \"mcagriaksoy\" / \"Database1\" / \"Database1\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34d8609-a7dd-4101-944e-f08ef155406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset dasmehdixtr: 2719it [00:01, 2191.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset dasmehdixtr_yolo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset dasmehdixtr: 3291it [00:00, 3845.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset dasmehdixtr_xml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset mcagriaksoy: 8025it [00:01, 4452.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset mcagriaksoy_yolo...\n",
      "\n",
      "Всего изображений: 5673\n",
      "Всего аннотаций: 6467\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    download_dataset(dataset, datasets_dir)\n",
    "    process_dataset(dataset, IMAGES_DIR, ANNOTATIONS_DIR)\n",
    "    print(f\"Processing Dataset {dataset.name}_{dataset.meta_type}...\")\n",
    "\n",
    "# Проверка результатов\n",
    "image_files = list(IMAGES_DIR.iterdir())\n",
    "annotation_files = list(ANNOTATIONS_DIR.iterdir())\n",
    "\n",
    "print(f\"\\nВсего изображений: {len(image_files)}\")\n",
    "print(f\"Всего аннотаций: {len(annotation_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c906b7-da5a-4d00-aa90-a75b07f109bb",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64881ee9-1a7f-45b3-8acc-2b1a82193e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms\n",
    "import json\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b945d87-dbc8-4ea6-ac98-b7d697d74dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 61\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     63\u001b[0m     val_ap \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data_loader, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/projects/study/hse/hse-dd/venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/study/hse/hse-dd/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/study/hse/hse-dd/venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images_dir = str(IMAGES_DIR)\n",
    "labels_dir = str(ANNOTATIONS_DIR)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = YoloDataset(images_dir, labels_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "def train(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    return losses.item()\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    aps = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for target, output in zip(targets, outputs):\n",
    "                gt_boxes = target['boxes'].cpu().numpy()\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                aps.append(average_precision_score(gt_boxes, pred_boxes, pred_scores))\n",
    "    return sum(aps) / len(aps)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, optimizer, train_loader, device)\n",
    "    lr_scheduler.step()\n",
    "    val_ap = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss}, Val mAP: {val_ap:.4f}\")\n",
    "torch.save(model.state_dict(), \"faster_rcnn_yolo.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6738cd-8f97-4f17-8bdc-fcea2ae7cb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
